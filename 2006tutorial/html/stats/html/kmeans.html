<html><head><title>R: K-Means Clustering</title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1">
<link rel="stylesheet" type="text/css" href="../../R.css">
</head><body>

<table width="100%" summary="page for kmeans {stats}"><tr><td>kmeans {stats}</td><td align="right">R Documentation</td></tr></table>
<h2>K-Means Clustering</h2>


<h3>Description</h3>

<p>
Perform k-means clustering on a data matrix.
</p>


<h3>Usage</h3>

<pre>
kmeans(x, centers, iter.max = 10, nstart = 1,
       algorithm = c("Hartigan-Wong", "Lloyd", "Forgy", "MacQueen"))
</pre>


<h3>Arguments</h3>

<table summary="R argblock">
<tr valign="top"><td><code>x</code></td>
<td>
A numeric matrix of data, or an object that can be coerced to
such a matrix (such as a numeric vector or a data frame with all
numeric columns).</td></tr>
<tr valign="top"><td><code>centers</code></td>
<td>
Either the number of clusters or a set of initial
(distinct) cluster centres.  If a number, a random set of (distinct)
rows in <code>x</code> is chosen as the initial centres.</td></tr>
<tr valign="top"><td><code>iter.max</code></td>
<td>
The maximum number of iterations allowed.</td></tr>
<tr valign="top"><td><code>nstart</code></td>
<td>
If <code>centers</code> is a number, how many random sets
should be chosen?</td></tr>
<tr valign="top"><td><code>algorithm</code></td>
<td>
character: may be abbreviated.</td></tr>
</table>

<h3>Details</h3>

<p>
The data given by <code>x</code> is clustered by the <i>k</i>-means method,
which aims to partition the points into <i>k</i> groups such that the
sum of squares from points to the assigned cluster centres is minimized.
At the minimum, all cluster centres are at the mean of their Voronoi
sets (the set of data points which are nearest to the cluster centre).
</p>
<p>
The algorithm of Hartigan and Wong (1979) is used by default.  Note
that some authors use <i>k</i>-means to refer to a specific algorithm
rather than the general method: most commonly the algorithm given by
MacQueen (1967) but sometimes that given by Lloyd (1957) and Forgy
(1965). The Hartigan&ndash;Wong algorithm generally does a better job than
either of those, but trying several random starts is often
recommended.
</p>
<p>
Except for the Lloyd&ndash;Forgy method, <i>k</i> clusters will always be
returned if a number is specified.
If an initial matrix of centres is supplied, it is possible that
no point will be closest to one or more centres, which is currently
an error for the Hartigan&ndash;Wong method.
</p>


<h3>Value</h3>

<p>
An object of class <code>"kmeans"</code> which is a list with components:
</p>
<table summary="R argblock">
<tr valign="top"><td><code>cluster</code></td>
<td>
A vector of integers indicating the cluster to which each point is
allocated.
</td></tr>
<tr valign="top"><td><code>centers</code></td>
<td>
A matrix of cluster centres.</td></tr>
<tr valign="top"><td><code>withinss</code></td>
<td>
The within-cluster sum of squares for each cluster.</td></tr>
<tr valign="top"><td><code>size</code></td>
<td>
The number of points in each cluster.</td></tr>
</table>
<p>

<br>
There is a <code>print</code> method for this class.</p>

<h3>References</h3>

<p>
Forgy, E. W. (1965) Cluster analysis of multivariate data:
efficiency vs interpretability of classifications.
<EM>Biometrics</EM> <B>21</B>, 768&ndash;769.
</p>
<p>
Hartigan, J. A. and Wong, M. A. (1979).
A K-means clustering algorithm.
<EM>Applied Statistics</EM> <B>28</B>, 100&ndash;108.
</p>
<p>
Lloyd, S. P. (1957, 1982)  Least squares quantization in PCM.
Technical Note, Bell Laboratories.  Published in 1982 in
<EM>IEEE Transactions on Information Theory</EM> <B>28</B>, 128&ndash;137.
</p>
<p>
MacQueen, J. (1967)  Some methods for classification and analysis of
multivariate observations. In <EM>Proceedings of the Fifth Berkeley
Symposium on  Mathematical Statistics and  Probability</EM>,
eds L. M. Le Cam &amp; J. Neyman,
<B>1</B>, pp. 281&ndash;297. Berkeley, CA: University of California Press.
</p>


<h3>Examples</h3>

<pre>
# a 2-dimensional example
x &lt;- rbind(matrix(rnorm(100, sd = 0.3), ncol = 2),
           matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2))
colnames(x) &lt;- c("x", "y")
(cl &lt;- kmeans(x, 2))
plot(x, col = cl$cluster)
points(cl$centers, col = 1:2, pch = 8, cex=2)

## random starts do help here with too many clusters
(cl &lt;- kmeans(x, 5, nstart = 25))
plot(x, col = cl$cluster)
points(cl$centers, col = 1:5, pch = 8)
</pre>



<hr><div align="center">[Package <em>stats</em> version 2.1.0 <a href="00Index.html">Index]</a></div>

</body></html>
